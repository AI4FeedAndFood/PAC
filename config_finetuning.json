{
    "model_config": {
        "_base_model": "unsloth/llama-3-8b-Instruct-bnb-4bit",
        "base_model": "/content/drive/MyDrive/Data/set2-pdf-bis/model7", 
        "new_model" : false, 
        "save_model_path": "/content/drive/MyDrive/Data/models_baptiste/model_test",
        "save_on_dick" : true,
        "push_on_hub": false,
        "finetuned_model":"Baprick/llama3_ocr_value_0.2.2", 
        "max_seq_length": 8192, 
        "dtype":null,
        "load_in_4bit": true
    },
    "datasets" : {
        "dataset_train_name" : "Baprick/llama3_ocr_value_0.2.2",
        "dataset_test_name" : "/content/drive/MyDrive/Data/set-2-pdf-quatro/dataset llama",
        "input_field": "prompt",
        "split" : "train"
    },
    "lora_config": {
      "r": 8,
      "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
      "lora_alpha":8, 
      "lora_dropout":0, 
      "bias":"None",
      "use_gradient_checkpointing":true, 
      "use_rslora":false,
      "use_dora":false, 
      "loftq_config":null 
    },
    "training_config": {
        "per_device_train_batch_size": 1, 
        "gradient_accumulation_steps": 4, 
        "warmup_steps": 5,
        "max_steps":0, 
        "num_train_epochs": 3,
        "learning_rate": 2e-4, 
        "logging_steps": 1, 
        "optim" :"adamw_8bit", 
        "weight_decay" : 0.01,  
        "lr_scheduler_type": "linear",
        "seed" : 42, 
        "output_dir" : "outputs"
    }
}
